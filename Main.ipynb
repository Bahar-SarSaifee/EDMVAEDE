{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eafd5cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 11:13:15.772094: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-26 11:13:15.806397: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-26 11:13:15.807096: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-26 11:13:16.464865: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "from time import time\n",
    "import Nmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "import tensorflow as tf\n",
    "from pydotplus import graphviz\n",
    "from tensorflow.keras.utils import plot_model, model_to_dot\n",
    "\n",
    "from time import time\n",
    "import platform\n",
    "from sklearn.metrics import log_loss\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, Conv2DTranspose, Flatten, Reshape, Conv3D, Conv3DTranspose, MaxPooling2D, Dropout, GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Layer, InputSpec, Input, Dense, Multiply, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.regularizers import Regularizer, l1, l2, l1_l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from math import log\n",
    "\n",
    "from keras_preprocessing import image\n",
    "from PIL import Image\n",
    "from numpy import hstack\n",
    "from scipy import misc\n",
    "import matplotlib\n",
    "import scipy.io as scio\n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path_data = './data'\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb548aa",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0bb4b2",
   "metadata": {},
   "source": [
    "### Fashion_MV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76180f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fashion_MV():\n",
    "    a = 0\n",
    "    if a == 1:\n",
    "        from tensorflow.keras.datasets import fashion_mnist  # this requires keras>=2.0.9\n",
    "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "        x1 = x_test\n",
    "        y = y_test\n",
    "        x2 = np.copy(x1)\n",
    "        x3 = np.copy(x1)\n",
    "        x4 = np.copy(x1)\n",
    "        for i in range(len(y)):\n",
    "            xb = np.where(y_train == y[i])\n",
    "            xb = xb[0][0:2000]\n",
    "            rand = np.random.randint(0, len(xb), 1)\n",
    "            x2[i] = x_train[xb[rand]]\n",
    "        for i in range(len(y)):\n",
    "            xb = np.where(y_train == y[i])\n",
    "            xb = xb[0][2000:4000]\n",
    "            rand = np.random.randint(0, len(xb), 1)\n",
    "            x3[i] = x_train[xb[rand]]\n",
    "        for i in range(len(y)):\n",
    "            xb = np.where(y_train == y[i])\n",
    "            xb = xb[0][4000:6000]\n",
    "            rand = np.random.randint(0, len(xb), 1)\n",
    "            x4[i] = x_train[xb[rand]]\n",
    "        # x1 = x1.reshape([-1, 28, 28, 1]) / 255.0\n",
    "        x1 = x4.reshape([-1, 28, 28, 1]) / 255.0\n",
    "        x2 = x2.reshape([-1, 28, 28, 1]) / 255.0\n",
    "        x3 = x3.reshape([-1, 28, 28, 1]) / 255.0\n",
    "        # The similar way of MNIST-USPS to construct Fashion-MV\n",
    "        scio.savemat(path_data + '/3V_Fashion_MV.mat', {'X1': x1, 'X2': x2, 'X3': x3, 'Y': y})\n",
    "    data = scio.loadmat(path_data + \"/3V_Fashion_MV.mat\")\n",
    "    x1 = data['X1']\n",
    "    x2 = data['X2']\n",
    "    x3 = data['X3']\n",
    "    Y = data['Y'][0]\n",
    "    # ge = np.random.randint(0, len(x1), 1, dtype=int)\n",
    "    # image1 = np.reshape(x1[ge], (28, 28))\n",
    "    # image2 = np.reshape(x2[ge], (28, 28))\n",
    "    # image3 = np.reshape(x3[ge], (28, 28))\n",
    "    # print(Y[ge][0])\n",
    "    # plt.figure('Fmnist-v1')\n",
    "    # plt.imshow(image1)\n",
    "    # plt.show()\n",
    "    # plt.figure('Fmnist-v2')\n",
    "    # plt.imshow(image2)\n",
    "    # plt.show()\n",
    "    # plt.figure('Fmnist-v3')\n",
    "    # plt.imshow(image3)\n",
    "    # plt.show()\n",
    "    print(x1.shape)\n",
    "    print(x2.shape)\n",
    "    print(x3.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    return [x1, x2, x3], Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388afe5",
   "metadata": {},
   "source": [
    "### MNIST-USPS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41dec945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_MNIST_USPS_From_COMIC():\n",
    "    data = 0\n",
    "    if data == 1:\n",
    "        x = scio.loadmat(path_data + \"/MNIST-USPS.mat\")\n",
    "        print(x)\n",
    "        x1 = x['X1']\n",
    "        x2 = x['X2']\n",
    "        Y = x['Y']\n",
    "        print(x1.shape)\n",
    "        print(x2.shape)\n",
    "        print(Y.shape)\n",
    "        print(x1[0])\n",
    "        print(x2[0])\n",
    "        print(Y[0])\n",
    "        x1 = x1.reshape((5000, 28, 28))\n",
    "        x2 = x2.reshape((5000, 16, 16), order='A')\n",
    "        print(Y)\n",
    "        Y = Y[0].reshape(5000,)\n",
    "        print(Y)\n",
    "        xu_reshape = np.zeros([len(x2), 28, 28], dtype=float)\n",
    "        for i in range(len(x2)):\n",
    "            for x in range(16):\n",
    "                for y in range(16):\n",
    "                    xu_reshape[i][x + 6][y + 6] = x2[i][x][y]\n",
    "\n",
    "        print(x1.shape)\n",
    "        print(xu_reshape.shape)\n",
    "        print(Y.shape)\n",
    "        z = np.linspace(0, len(Y) - 1, len(Y), dtype=int)\n",
    "        np.random.shuffle(z)\n",
    "        # print(z)\n",
    "        # print(y_label)\n",
    "        x_data_m = x1\n",
    "        x_data_u = xu_reshape\n",
    "        y_label = Y\n",
    "        x_shuffle_m = np.copy(x_data_m)\n",
    "        x_shuffle_u = np.copy(x_data_u)\n",
    "        y_shuffle = np.copy(y_label)\n",
    "        for i in range(len(y_label)):\n",
    "            x_shuffle_m[i] = x_data_m[z[i]]\n",
    "            x_shuffle_u[i] = x_data_u[z[i]]\n",
    "            y_shuffle[i] = y_label[z[i]]\n",
    "        x_shuffle_m = x_shuffle_m.reshape([-1, 28, 28, 1])\n",
    "        x_shuffle_u = x_shuffle_u.reshape([-1, 28, 28, 1])/255\n",
    "        print(x_shuffle_m.shape)\n",
    "        print(x_shuffle_u.shape)\n",
    "        print(y_shuffle.shape)\n",
    "        print(x_shuffle_m[0])\n",
    "        print(x_shuffle_u[0])\n",
    "        # print(y_shuffle[0])\n",
    "        scio.savemat(path_data + '/2V_MNIST_USPS.mat', {'X1': x_shuffle_m, 'X2': x_shuffle_u, 'Y': y_shuffle})\n",
    "    data = scio.loadmat(path_data + \"/2V_MNIST_USPS.mat\")\n",
    "    x1 = data['X1']\n",
    "    x2 = data['X2']\n",
    "    Y = data['Y'][0]\n",
    "    ge = np.random.randint(0, len(x1), 1, dtype=int)\n",
    "    image1 = np.reshape(x1[ge], (28, 28))\n",
    "    image2 = np.reshape(x2[ge], (28, 28))\n",
    "#     print(Y[ge][0])\n",
    "#     plt.figure('Mnist')\n",
    "#     plt.imshow(image1)\n",
    "#     plt.show()\n",
    "#     plt.figure('USPS')\n",
    "#     plt.imshow(image2)\n",
    "#     plt.show()\n",
    "    print(x1.shape)\n",
    "    print(x2.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    return [x1, x2], Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ddbc8",
   "metadata": {},
   "source": [
    "### Caltech101_20 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20f86f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Caltech101_20():\n",
    "    data = 0\n",
    "    if data == 1:\n",
    "        import scipy.io as scio\n",
    "        data = scio.loadmat(path_data + \"/Caltech101-20.mat\")\n",
    "        Y = data['Y'] - 1\n",
    "        # print(Y.shape)\n",
    "        X = data['X']\n",
    "        print(X[0][0].shape)\n",
    "        print(X[0][1].shape)\n",
    "        print(X[0][2].shape)\n",
    "        print(X[0][3].shape)\n",
    "        print(X[0][4].shape)\n",
    "        print(X[0][5].shape)\n",
    "        x1 = X[0][0]\n",
    "        x2 = X[0][1]\n",
    "        x3 = X[0][2]\n",
    "        x4 = X[0][3]\n",
    "        x5 = X[0][4]\n",
    "        x6 = X[0][5]\n",
    "        t = np.linspace(0, Y.shape[0] - 1, Y.shape[0], dtype=int)\n",
    "        print(t)\n",
    "        import random\n",
    "        random.shuffle(t)\n",
    "        # np.save(\"./Caltech101_20_t.npy\", t)\n",
    "        t = np.load(\"./Caltech101_20_t.npy\")\n",
    "        print(t)\n",
    "        xx1 = np.copy(x1)\n",
    "        xx2 = np.copy(x2)\n",
    "        xx3 = np.copy(x3)\n",
    "        xx4 = np.copy(x4)\n",
    "        xx5 = np.copy(x5)\n",
    "        xx6 = np.copy(x6)\n",
    "        YY = np.copy(Y)\n",
    "        for i in range(Y.shape[0]):\n",
    "            x1[i] = xx1[t[i]]\n",
    "            x2[i] = xx2[t[i]]\n",
    "            x3[i] = xx3[t[i]]\n",
    "            x4[i] = xx4[t[i]]\n",
    "            x5[i] = xx5[t[i]]\n",
    "            x6[i] = xx6[t[i]]\n",
    "            Y[i] = YY[t[i]]\n",
    "        from sklearn import preprocessing\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        x1 = min_max_scaler.fit_transform(x1)\n",
    "        x2 = min_max_scaler.fit_transform(x2)\n",
    "        x3 = min_max_scaler.fit_transform(x3)\n",
    "        x4 = min_max_scaler.fit_transform(x4)\n",
    "        x5 = min_max_scaler.fit_transform(x5)\n",
    "        x6 = min_max_scaler.fit_transform(x6)\n",
    "        print(x1[0])\n",
    "        Y = Y.reshape(Y.shape[0])\n",
    "        print(Y.shape)\n",
    "        scio.savemat(path_data + '/6V_Caltech101_20.mat', {'X1': x1, 'X2': x2, 'X3': x3, 'X4': x4, 'X5': x5, 'X6': x6, 'Y': Y})\n",
    "    import scipy.io as scio\n",
    "    data = scio.loadmat(path_data + \"/6V_Caltech101_20.mat\")\n",
    "    x1 = data['X1']\n",
    "    x2 = data['X2']\n",
    "    x3 = data['X3']\n",
    "    x4 = data['X4']\n",
    "    x5 = data['X5']\n",
    "    x6 = data['X6']\n",
    "    Y = data['Y'][0]\n",
    "    print(x1.shape)\n",
    "    print(x2.shape)\n",
    "    print(x3.shape)\n",
    "    print(x4.shape)\n",
    "    print(x5.shape)\n",
    "    print(x6.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    return [x1, x2, x3, x4, x5, x6], Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a1a87",
   "metadata": {},
   "source": [
    "### BDGP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2423606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BDGP():\n",
    "    data = scio.loadmat(path_data + \"/2V_BDGP.mat\")\n",
    "    x1 = data['X1']\n",
    "    x2 = data['X2']\n",
    "    Y = data['Y'][0]\n",
    "    print(x1.shape)\n",
    "    print(x2.shape)\n",
    "    print(Y.shape)\n",
    "    return [x1, x2], Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c7b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_conv(dataset):\n",
    "    print(\"load:\", dataset)\n",
    "    if dataset == 'Fashion_MV':                   # Fashion-10K-3views\n",
    "        return Fashion_MV()\n",
    "    elif dataset == 'MNIST_USPS':                 # MNIST-USPS\n",
    "        return Get_MNIST_USPS_From_COMIC()\n",
    "    elif dataset == 'Caltech101_20':              # Caltech101_20\n",
    "        return Caltech101_20()\n",
    "    elif dataset == 'BDGP':                       # BDGP\n",
    "        return BDGP()\n",
    "    else:\n",
    "        raise ValueError('Not defined for loading %s' % dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaedba4f",
   "metadata": {},
   "source": [
    "## SDMVC File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da682ca",
   "metadata": {},
   "source": [
    "### Fully connected Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb478b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FAE(dims, act='relu', view=1):\n",
    "    \n",
    "    \n",
    "    n_stacks = len(dims) - 1\n",
    "    \n",
    "    init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n",
    "\n",
    "    input_name = 'v'+str(view)+'_'\n",
    "    \n",
    "    # input\n",
    "    x = Input(shape=(dims[0],), name='input' + str(view))\n",
    "\n",
    "    h = x\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name=input_name+'encoder_%d' % i)(h)\n",
    " \n",
    "    # hidden layer\n",
    "    h = Dense(100, kernel_initializer=init, name='embedding' + str(view))(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name=input_name+'decoder_%d' % i)(y)\n",
    "\n",
    "    # output\n",
    "    y = Dense(dims[0], kernel_initializer=init, name=input_name+'decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y, name=input_name+'Fae'), Model(inputs=x, outputs=h, name=input_name+'Fencoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e0852",
   "metadata": {},
   "source": [
    "### Multi-view Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41274ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MAE(view=6, filters=[32, 64, 128, 10], view_shape = [1, 2, 3]):\n",
    "\n",
    "def MAE(view=6, filters=[32, 64, 128, 100], view_shape = [1, 2, 3]):\n",
    "\n",
    "    if len(view_shape[0]) == 1:\n",
    "        typenet = 'f-f'          # Fully connected networks\n",
    "    else:\n",
    "        typenet = 'c-c'          # Convolution networks\n",
    "\n",
    "    if typenet == 'c-c':\n",
    "        input1_shape = view_shape[0]\n",
    "        input2_shape = view_shape[1]\n",
    "\n",
    "        if input1_shape[0] % 8 == 0:\n",
    "            pad1 = 'same'\n",
    "        else:\n",
    "            pad1 = 'valid'\n",
    "        print(\"----------------------\")\n",
    "        print(filters)\n",
    "        input1 = Input(input1_shape, name='input1')\n",
    "        x = Conv2D(filters[0], 5, strides=2, padding='same', activation='relu', name='conv1_v1')(input1)\n",
    "        x = Conv2D(filters[1], 5, strides=2, padding='same', activation='relu', name='conv2_v1')(x)\n",
    "        x = Conv2D(filters[2], 3, strides=2, padding=pad1, activation='relu', name='conv3_v1')(x)\n",
    "        x = Flatten(name='Flatten1')(x)\n",
    "        x1 = Dense(units=filters[3], name='embedding1')(x)\n",
    "        x = Dense(units=filters[2]*int(input1_shape[0]/8)*int(input1_shape[0]/8), activation='relu',\n",
    "                  name='Dense1')(x1)\n",
    "        x = Reshape((int(input1_shape[0]/8), int(input1_shape[0]/8), filters[2]), name='Reshape1')(x)\n",
    "        x = Conv2DTranspose(filters[1], 3, strides=2, padding=pad1, activation='relu', name='deconv3_v1')(x)\n",
    "        x = Conv2DTranspose(filters[0], 5, strides=2, padding='same', activation='relu', name='deconv2_v1')(x)\n",
    "        x = Conv2DTranspose(input1_shape[2], 5, strides=2, padding='same', name='deconv1_v1')(x)\n",
    "\n",
    "        input2 = Input(input2_shape, name='input2')\n",
    "        xn = Conv2D(filters[0], 5, strides=2, padding='same', activation='relu', name='conv1_v2')(input2)\n",
    "        xn = Conv2D(filters[1], 5, strides=2, padding='same', activation='relu', name='conv2_v2')(xn)\n",
    "        xn = Conv2D(filters[2], 3, strides=2, padding=pad1, activation='relu', name='conv3_v2')(xn)\n",
    "        xn = Flatten(name='Flatten2')(xn)\n",
    "        x2 = Dense(units=filters[3], name='embedding2')(xn)\n",
    "        xn = Dense(units=filters[2] * int(input2_shape[0] / 8) * int(input2_shape[0] / 8), activation='relu',\n",
    "                   name='Dense2')(x2)\n",
    "        xn = Reshape((int(input2_shape[0] / 8), int(input2_shape[0] / 8), filters[2]), name='Reshape2')(xn)\n",
    "        xn = Conv2DTranspose(filters[1], 3, strides=2, padding=pad1, activation='relu', name='deconv3_v2')(xn)\n",
    "        xn = Conv2DTranspose(filters[0], 5, strides=2, padding='same', activation='relu', name='deconv2_v2')(xn)\n",
    "        xn = Conv2DTranspose(input2_shape[2], 5, strides=2, padding='same', name='deconv1_v2')(xn)\n",
    "        encoder1 = Model(inputs=input1, outputs=x1)\n",
    "        encoder2 = Model(inputs=input2, outputs=x2)\n",
    "        ae1 = Model(inputs=input1, outputs=x)\n",
    "        ae2 = Model(inputs=input2, outputs=xn)\n",
    "\n",
    "        if view == 2:\n",
    "            return [ae1, ae2], [encoder1, encoder2]\n",
    "        else:\n",
    "            input3_shape = view_shape[2]\n",
    "            input3 = Input(input3_shape, name='input3')\n",
    "            xr = Conv2D(filters[0], 5, strides=2, padding='same', activation='relu', name='conv1_v3')(input3)\n",
    "            xr = Conv2D(filters[1], 5, strides=2, padding='same', activation='relu', name='conv2_v3')(xr)\n",
    "            xr = Conv2D(filters[2], 3, strides=2, padding=pad1, activation='relu', name='conv3_v3')(xr)\n",
    "            xr = Flatten(name='Flatten3')(xr)\n",
    "            x3 = Dense(units=filters[3], name='embedding3')(xr)\n",
    "            xr = Dense(units=filters[2] * int(input3_shape[0] / 8) * int(input3_shape[0] / 8), activation='relu',\n",
    "                       name='Dense3')(x3)\n",
    "            xr = Reshape((int(input3_shape[0] / 8), int(input3_shape[0] / 8), filters[2]), name='Reshape3')(xr)\n",
    "            xr = Conv2DTranspose(filters[1], 3, strides=2, padding=pad1, activation='relu', name='deconv3_v3')(xr)\n",
    "            xr = Conv2DTranspose(filters[0], 5, strides=2, padding='same', activation='relu', name='deconv2_v3')(xr)\n",
    "            xr = Conv2DTranspose(input2_shape[2], 5, strides=2, padding='same', name='deconv1_v3')(xr)\n",
    "\n",
    "            encoder3 = Model(inputs=input3, outputs=x3)\n",
    "            ae3 = Model(inputs=input3, outputs=xr)\n",
    "\n",
    "            return [ae1, ae2, ae3], [encoder1, encoder2, encoder3]\n",
    "\n",
    "    if typenet == 'f-f':\n",
    "        ae = []\n",
    "        encoder = []\n",
    "        for v in range(view):\n",
    "            ae_tmp, encoder_tmp = FAE(dims=[view_shape[v][0], 500, 500, 2000, 10], view=v + 1)\n",
    "            ae.append(ae_tmp)\n",
    "            encoder.append(encoder_tmp)\n",
    "        \n",
    "        return ae, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d9ac55",
   "metadata": {},
   "source": [
    "### Clustering Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e977c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2    \n",
    "        input_dim = input_shape.as_list()[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51093383",
   "metadata": {},
   "source": [
    "### Multi-view Discriminative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0677d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MvDEC(object):\n",
    "    def __init__(self, filters=[32, 64, 128, 10], view=6, n_clusters=20, alpha=1.0, view_shape = [1, 2, 3, 4, 5, 6]):\n",
    "        np.random.seed(10)\n",
    "        super(MvDEC, self).__init__()\n",
    "\n",
    "        self.view_shape = view_shape\n",
    "        self.filters = filters\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.pretrained = False\n",
    "        # prepare MvDEC model\n",
    "        self.view = len(view_shape)\n",
    "\n",
    "\n",
    "        self.AEs, self.encoders = MAE(view=self.view, filters=self.filters, view_shape=self.view_shape)\n",
    "    \n",
    "        Input = []\n",
    "        Output = []\n",
    "        Input_e = []\n",
    "        Output_e = []\n",
    "        clustering_layer = []\n",
    "\n",
    "        for v in range(self.view):\n",
    "            Input.append(self.AEs[v].input)\n",
    "            Output.append(self.AEs[v].output)\n",
    "            Input_e.append(self.encoders[v].input)\n",
    "            Output_e.append(self.encoders[v].output)\n",
    "            clustering_layer.append(ClusteringLayer(self.n_clusters, name='clustering'+str(v+1))(self.encoders[v].output))\n",
    "\n",
    "        self.autoencoder = Model(inputs=Input, outputs=Output)    # xin _ xout\n",
    "\n",
    "        self.encoder = Model(inputs=Input_e, outputs=Output_e)   # xin _ q\n",
    "\n",
    "        Output_m = []\n",
    "        for v in range(self.view):\n",
    "            Output_m.append(clustering_layer[v])\n",
    "            Output_m.append(Output[v])\n",
    "        self.model = Model(inputs=Input, outputs=Output_m)   # xin _ q _ xout\n",
    "\n",
    "    def pretrain(self, x, y, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp', verbose=0):\n",
    "\n",
    "        print('Begin pretraining: ', '-' * 60)\n",
    "        multi_loss = []\n",
    "        for view in range(len(x)):\n",
    "            multi_loss.append('mse')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss=multi_loss)\n",
    "        csv_logger = callbacks.CSVLogger(save_dir + '/T_pretrain_ae_log.csv')\n",
    "        save = '/ae_weights.h5'\n",
    "        cb = [csv_logger]\n",
    "        \n",
    "        if y is not None and verbose > 0:\n",
    "            \n",
    "            class PrintACC(callbacks.Callback):\n",
    "                def __init__(self, x, y, flag=1):\n",
    "                    self.x = x\n",
    "                    self.y = y\n",
    "                    self.flag = flag\n",
    "                    super(PrintACC, self).__init__()\n",
    "\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    time = 1    #  show k-means results on z\n",
    "                    if int(epochs / time) != 0 and (epoch+1) % int(epochs/time) != 0:\n",
    "                        # print(epoch)\n",
    "                        return\n",
    "                    view_name = 'embedding' + str(self.flag)\n",
    "                    feature_model = Model(self.model.input[self.flag - 1], self.model.get_layer(name=view_name).output)\n",
    "                    \n",
    "                    features = feature_model.predict(self.x)\n",
    "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20, n_jobs=4)\n",
    "                    y_pred = km.fit_predict(features)\n",
    "                    \n",
    "                    print('\\n' + ' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (Nmetrics.acc(self.y, y_pred), Nmetrics.nmi(self.y, y_pred)))\n",
    "\n",
    "            for view in range(len(x)):\n",
    "                cb.append(PrintACC(x[view], y, flag=view + 1))\n",
    "\n",
    "        # begin pretraining\n",
    "        t0 = time()\n",
    "        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb, verbose=verbose)\n",
    "\n",
    "        print('Pretraining time: ', time() - t0)\n",
    "        self.autoencoder.save_weights(save_dir + save)\n",
    "        print('Pretrained weights are saved to ' + save_dir + save)\n",
    "        self.pretrained = True\n",
    "        print('End pretraining: ', '-' * 60)\n",
    "\n",
    "    def load_weights(self, weights):  # load weights of models\n",
    "        self.model.load_weights(weights)\n",
    "\n",
    "    def predict_label(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        input_dic = {}\n",
    "        for view in range(len(x)):\n",
    "            input_dic.update({'input' + str(view+1): x[view]})\n",
    "        Q_and_X = self.model.predict(input_dic, verbose=0)\n",
    "        y_pred = []\n",
    "        for view in range(len(x)):\n",
    "            # print(view)\n",
    "            y_pred.append(Q_and_X[view*2].argmax(1))\n",
    "        \n",
    "        y_q = Q_and_X[(len(x)-1)*2]\n",
    "        for view in range(len(x) - 1):\n",
    "            y_q += Q_and_X[view*2]\n",
    "\n",
    "        # y_q = y_q/len(x)\n",
    "        y_mean_pred = y_q.argmax(1)\n",
    "        return y_pred, y_mean_pred\n",
    "\n",
    "    @staticmethod    \n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        # return q\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, optimizer='sgd', loss=['kld', 'mse'], loss_weights=[0.1, 1.0]):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, loss_weights=loss_weights)\n",
    "\n",
    "    def train_on_batch(self, xin, yout, sample_weight=None):\n",
    "        return self.model.train_on_batch(xin, yout, sample_weight)\n",
    "\n",
    "    # DEMVC\n",
    "    def fit(self, arg, x, y, maxiter=2e4, batch_size=256, tol=1e-3, UpdateCoo=200, save_dir='./results/tmp'):\n",
    "        \n",
    "        print('Begin clustering:', '-' * 60)\n",
    "        print('Update Coo:', UpdateCoo)\n",
    "        save_interval = int(maxiter)  # only save the initial and final model\n",
    "        print('Save interval', save_interval)\n",
    "        \n",
    "        # Step 1: initialize cluster centers using k-means\n",
    "        t1 = time()\n",
    "        ting = time() - t1\n",
    "        print(ting)\n",
    "        \n",
    "        time_record = []\n",
    "        time_record.append(int(ting))\n",
    "        print(time_record)\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=100)\n",
    "        \n",
    "        input_dic = {}\n",
    "        for view in range(len(x)):\n",
    "            input_dic.update({'input' + str(view+1): x[view]})\n",
    "        features = self.encoder.predict(input_dic)\n",
    "\n",
    "        y_pred = []\n",
    "        center = []\n",
    "\n",
    "# --------------------------------------------\n",
    "        \n",
    "        c = 1\n",
    "        if c == 1:\n",
    "            for view in range(len(x)):\n",
    "                y_pred.append(kmeans.fit_predict(features[view]))\n",
    "                np.save('TC'+str(view+1)+'.npy', [kmeans.cluster_centers_])\n",
    "                center.append(np.load('TC'+str(view+1)+'.npy'))\n",
    "        else:\n",
    "            from numpy import hstack\n",
    "            from sklearn import preprocessing\n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            # n_features = []\n",
    "            # for view in range(len(x)):\n",
    "            #     n_features.append(min_max_scaler.fit_transform(features[view]))\n",
    "            #     # n_features.append(features[view])\n",
    "            # z = hstack(n_features)\n",
    "            z = hstack(features)\n",
    "            print(features[0].shape, len(x), z.shape)\n",
    "            y_pred.append(kmeans.fit_predict(z))\n",
    "            for view in range(len(x)-1):\n",
    "                y_pred.append(y_pred[0])\n",
    "            print(kmeans.cluster_centers_.shape)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            for view in range(len(x)):\n",
    "                b = 10*view\n",
    "                e = b + 10\n",
    "                np.save('TC'+str(view+1)+'.npy', [centers[:, b:e]])\n",
    "                center.append(np.load('TC'+str(view+1)+'.npy'))\n",
    "# --------------------------------------------\n",
    "\n",
    "        for view in range(len(x)):\n",
    "            acc = np.round(Nmetrics.acc(y, y_pred[view]), 5)\n",
    "            nmi = np.round(Nmetrics.nmi(y, y_pred[view]), 5)\n",
    "            vmea = np.round(Nmetrics.vmeasure(y, y_pred[view]), 5)\n",
    "            ari = np.round(Nmetrics.ari(y, y_pred[view]), 5)\n",
    "            print('Start-'+str(view+1)+': acc=%.5f, nmi=%.5f, v-measure=%.5f, ari=%.5f' % (acc, nmi, vmea, ari))\n",
    "\n",
    "        y_pred_last = []\n",
    "        y_pred_sp = []\n",
    "        for view in range(len(x)):\n",
    "            y_pred_last.append(y_pred[view])\n",
    "            y_pred_sp.append(y_pred[view])\n",
    "\n",
    "        for view in range(len(x)):\n",
    "            if arg.K12q == 0:\n",
    "                self.model.get_layer(name='clustering'+str(view+1)).set_weights(center[view])\n",
    "            else:\n",
    "                self.model.get_layer(name='clustering'+str(view+1)).set_weights(center[arg.K12q - 1])\n",
    "\n",
    "        # Step 2: deep clustering\n",
    "        # logging file\n",
    "        import csv\n",
    "        import os\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        logfile = open(save_dir + '/log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'nmi', 'vmea', 'ari', 'loss'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        index_array = np.arange(x[0].shape[0])\n",
    "        index = 0\n",
    "        \n",
    "        Loss = []\n",
    "        avg_loss = []\n",
    "        for view in range(len(x)):\n",
    "            Loss.append(0)\n",
    "            avg_loss.append(0)\n",
    "\n",
    "        flag = 1\n",
    "        \n",
    "        vf = arg.view_first\n",
    "\n",
    "        update_interval = arg.UpdateCoo\n",
    "        \n",
    "        for ite in range(int(maxiter)):  # fine-turing\n",
    "\n",
    "            if ite % update_interval == 0:\n",
    "\n",
    "                Q_and_X = self.model.predict(input_dic)\n",
    "                # Coo\n",
    "                for view in range(len(x)):\n",
    "                    y_pred_sp[view] = Q_and_X[view*2].argmax(1)\n",
    "\n",
    "                # print(flag, (flag % len(x)))\n",
    "                # view_num = len(x)\n",
    "                q_index = (flag + vf - 1) % len(x)\n",
    "                if q_index == 0:\n",
    "                    q_index = len(x)\n",
    "                p = self.target_distribution(Q_and_X[(q_index-1) * 2])  # q->p\n",
    "                # print(q_index)\n",
    "                flag += 1\n",
    "                print('Next corresponding: p' + str(q_index))\n",
    "\n",
    "                P = []\n",
    "                if arg.Coo == 1:\n",
    "                    for view in range(len(x)):\n",
    "                        P.append(p)\n",
    "                else:\n",
    "                    for view in range(len(x)):\n",
    "                        P.append(self.target_distribution(Q_and_X[view*2]))\n",
    "\n",
    "                ge = np.random.randint(0, x[0].shape[0], 1, dtype=int)\n",
    "                ge = int(ge)\n",
    "                print('Number of sample:' + str(ge))\n",
    "                for view in range(len(x)):\n",
    "                    for i in Q_and_X[view*2][ge]:\n",
    "                        print(\"%.3f  \" % i, end=\"\")\n",
    "                    print(\"\\n\")\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                for view in range(len(x)):\n",
    "                    avg_loss[view] = Loss[view] / update_interval\n",
    "                \n",
    "                for view in range(len(x)):\n",
    "                    Loss[view] = 0.\n",
    "\n",
    "                if y is not None:\n",
    "                    for num in range(self.n_clusters):\n",
    "                        same = np.where(y == num)   \n",
    "                        same = np.array(same)[0]\n",
    "                        Out = y_pred_sp[len(x)-1][same]\n",
    "                        for view in range(len(x)-1):\n",
    "                            Out += y_pred_sp[view][same]\n",
    "\n",
    "                        out = Out\n",
    "                        comp = y_pred_sp[0][same]\n",
    "                        \n",
    "                        for i in range(len(out)):\n",
    "                            if Out[i]/len(x) == comp[i]:\n",
    "                                out[i] = 0\n",
    "                            else:\n",
    "                                out[i] = 1\n",
    "                        if (len(out) != 0):        # Simply calculate the scale of the alignment\n",
    "                            print('%d, %.2f%%, %d' % (num,len(np.array(np.where(out == 0))[0]) * 100/len(out),len(same)))\n",
    "                        else:\n",
    "                            print('%d, %.2f%%. %d' % (num, 0, len(same)))\n",
    "\n",
    "                    for view in range(len(x)):\n",
    "                        acc = np.round(Nmetrics.acc(y, y_pred_sp[view]), 5)\n",
    "                        nmi = np.round(Nmetrics.nmi(y, y_pred_sp[view]), 5)\n",
    "                        vme = np.round(Nmetrics.vmeasure(y, y_pred_sp[view]), 5)\n",
    "                        ari = np.round(Nmetrics.ari(y, y_pred_sp[view]), 5)\n",
    "                        logdict = dict(iter=ite, nmi=nmi, vmea=vme, ari=ari, loss=avg_loss[view])\n",
    "                        logwriter.writerow(logdict)\n",
    "                        logfile.flush()\n",
    "                        print('V'+str(view+1)+'-Iter %d: acc=%.5f, nmi=%.5f, v-measure=%.5f, ari=%.5f; loss=%.5f' % (\n",
    "                        ite, acc, nmi, vme, ari, avg_loss[view]))\n",
    "\n",
    "                    ting = time() - t1\n",
    "\n",
    "            # train on batch\n",
    "            idx = index_array[index * batch_size: min((index + 1) * batch_size, x[0].shape[0])]\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for view in range(len(x)):\n",
    "                x_batch.append(x[view][idx])\n",
    "                y_batch.append(P[view][idx])\n",
    "                y_batch.append(x[view][idx])\n",
    "\n",
    "            tmp = self.train_on_batch(xin=x_batch, yout=y_batch)   # [y, xn, y, x]\n",
    "            \n",
    "            for view in range(len(x)):\n",
    "                Loss[view] += tmp[2*view+1]\n",
    "            \n",
    "            index = index + 1 if (index + 1) * batch_size <= x[0].shape[0] else 0\n",
    "            # ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/model_final.h5')\n",
    "        self.autoencoder.save_weights(save_dir + '/pre_model.h5')\n",
    "        print('Clustering time: %ds' % (time() - t1))\n",
    "        print('End clustering:', '-' * 60)\n",
    "        \n",
    "        Q_and_X = self.model.predict(input_dic)\n",
    "        y_pred = []\n",
    "        for view in range(len(x)):\n",
    "            y_pred.append(Q_and_X[view*2].argmax(1))\n",
    "        \n",
    "        y_q = Q_and_X[(len(x)-1)*2]\n",
    "        for view in range(len(x) - 1):\n",
    "            y_q += Q_and_X[view*2]\n",
    "        # y_q = y_q/len(x)\n",
    "        y_mean_pred = y_q.argmax(1)\n",
    "        return y_pred, y_mean_pred\n",
    "\n",
    "    # SDMVC\n",
    "    def new_fit(self, arg, x, y, maxiter=2e4, batch_size=256, tol=1e-3, UpdateCoo=200, save_dir='./results/tmp'):\n",
    "        \n",
    "        print('Begin clustering:', '-' * 60)\n",
    "        print('Update Coo:', UpdateCoo)\n",
    "        save_interval = int(maxiter)  # only save the initial and final model\n",
    "        print('Save interval', save_interval)\n",
    "        \n",
    "        # Step 1: initialize cluster centers using k-means\n",
    "        t1 = time()\n",
    "        ting = time() - t1\n",
    "        \n",
    "        # print(ting)\n",
    "        time_record = []\n",
    "        time_record.append(int(ting))\n",
    "        \n",
    "        # print(time_record)\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=100)\n",
    "\n",
    "        input_dic = {}\n",
    "        for view in range(len(x)):\n",
    "            input_dic.update({'input' + str(view + 1): x[view]})\n",
    "        features = self.encoder.predict(input_dic)\n",
    "\n",
    "        if len(x) <= 3:      # small trick: less view, more times to over arg.AR, so as to get high Aligned Rate\n",
    "            arg.ARtime = 3\n",
    "        else:\n",
    "            arg.ARtime = 2\n",
    "\n",
    "        y_pred = []\n",
    "        center = []\n",
    "\n",
    "        from numpy import hstack\n",
    "        from sklearn import preprocessing\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# --------------------------------------------\n",
    "        c = 1\n",
    "        if c == 1:\n",
    "            for view in range(len(x)):\n",
    "                y_pred.append(kmeans.fit_predict(features[view]))\n",
    "                # np.save('TC' + str(view + 1) + '.npy', [kmeans.cluster_centers_])\n",
    "                # center.append(np.load('TC' + str(view + 1) + '.npy'))\n",
    "                center.append([kmeans.cluster_centers_])\n",
    "\n",
    "        elif c == 2:\n",
    "            n_features = []\n",
    "            for view in range(len(x)):\n",
    "                # n_features.append(min_max_scaler.fit_transform(features[view]))\n",
    "                n_features.append(features[view])\n",
    "            z = hstack(n_features)\n",
    "            print(features[0].shape, len(x), z.shape)\n",
    "            y_pred.append(kmeans.fit_predict(z))\n",
    "            for view in range(len(x) - 1):\n",
    "                y_pred.append(y_pred[0])\n",
    "            print(kmeans.cluster_centers_.shape)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            # print(self.new_P(z, centers))\n",
    "            new_P = self.new_P(z, centers)\n",
    "            print(new_P.argmax(1))\n",
    "            print(y_pred[0])\n",
    "            for view in range(len(x)):\n",
    "                b = 10 * view\n",
    "                e = b + 10\n",
    "                np.save('TC' + str(view + 1) + '.npy', [centers[:, b:e]])\n",
    "                center.append(np.load('TC' + str(view + 1) + '.npy'))\n",
    "        else:\n",
    "            for view in range(len(x)):\n",
    "                y_pred.append(kmeans.fit_predict(features[view]))\n",
    "            print(\"random\")\n",
    "        # --------------------------------------------\n",
    "\n",
    "        for view in range(len(x)):\n",
    "            acc = np.round(Nmetrics.acc(y, y_pred[view]), 5)\n",
    "            nmi = np.round(Nmetrics.nmi(y, y_pred[view]), 5)\n",
    "            vmea = np.round(Nmetrics.vmeasure(y, y_pred[view]), 5)\n",
    "            ari = np.round(Nmetrics.ari(y, y_pred[view]), 5)\n",
    "            print('Start-' + str(view + 1) + ': acc=%.5f, nmi=%.5f, v-measure=%.5f, ari=%.5f' % (acc, nmi, vmea, ari))\n",
    "\n",
    "        y_pred_last = []\n",
    "        y_pred_sp = []\n",
    "        for view in range(len(x)):\n",
    "            y_pred_last.append(y_pred[view])\n",
    "            y_pred_sp.append(y_pred[view])\n",
    "\n",
    "        for view in range(len(x)):\n",
    "            # break\n",
    "            if arg.K12q == 0:\n",
    "                self.model.get_layer(name='clustering' + str(view + 1)).set_weights(center[view])\n",
    "            else:\n",
    "                self.model.get_layer(name='clustering' + str(view + 1)).set_weights(center[arg.K12q - 1])\n",
    "\n",
    "        # Step 2: deep clustering\n",
    "        # logging file\n",
    "        import csv\n",
    "        import os\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        logfile = open(save_dir + '/log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'nmi', 'vmea', 'ari', 'loss'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        index_array = np.arange(x[0].shape[0])\n",
    "        index = 0\n",
    "\n",
    "        Loss = []\n",
    "        avg_loss = []\n",
    "        kl_loss = []\n",
    "        for view in range(len(x)):\n",
    "            Loss.append(0)\n",
    "            avg_loss.append(0)\n",
    "            kl_loss.append(100000)\n",
    "\n",
    "        update_interval = arg.UpdateCoo\n",
    "        center_init = 0\n",
    "        alignment = 0\n",
    "        alignment_large = 0\n",
    "\n",
    "        ACC = []\n",
    "        NMI = []\n",
    "        ARI = []\n",
    "        vACC = []\n",
    "        vNMI = []\n",
    "        vARI = []\n",
    "        Rate = []\n",
    "        MVKLLoss = []\n",
    "        ite = 0\n",
    "        while True:\n",
    "            if ite % update_interval == 0:\n",
    "                print('\\n')\n",
    "                for view in range(len(x)):\n",
    "                    avg_loss[view] = Loss[view] / update_interval\n",
    "                    kl_loss[view] = kl_loss[view] / update_interval\n",
    "\n",
    "                Q_and_X = self.model.predict(input_dic)\n",
    "                # Coo\n",
    "                for view in range(len(x)):\n",
    "                    # print(Q_and_X[view * 2][0])\n",
    "                    y_pred_sp[view] = Q_and_X[view * 2].argmax(1)\n",
    "\n",
    "                features = self.encoder.predict(input_dic)\n",
    "                mu = []\n",
    "                for view in range(len(x)):\n",
    "                    muu = self.model.get_layer(name='clustering' + str(view + 1)).get_weights()\n",
    "                    # print(muu)\n",
    "                    mu.append(muu)\n",
    "\n",
    "                # np.save(save_dir + '/Features/' + str(ite) + '.npy', features)\n",
    "                # np.save(save_dir + '/Mu/' + str(ite) + '.npy', mu)\n",
    "                # print(features[0][0])\n",
    "                # print(features[1][0])\n",
    "\n",
    "                if len(x) >= 6:\n",
    "                    scaler = 1\n",
    "                else:\n",
    "                    scaler = 0\n",
    "                # print(\"scaler ? :\"+str(scaler))\n",
    "                # if views' number is too many (eg, >= 6), we can scale the features to [0,1] to build global features\n",
    "                if scaler == 1:\n",
    "                    n_features = []\n",
    "                    for view in range(len(x)):\n",
    "                        n_features.append(min_max_scaler.fit_transform(features[view]))\n",
    "                    z = hstack(n_features)\n",
    "                else:\n",
    "                    z = hstack(features)\n",
    "\n",
    "                kmean = KMeans(n_clusters=self.n_clusters, n_init=100)\n",
    "                y_pred = kmean.fit_predict(z)     # k-means on global features\n",
    "                # print(kmeans.cluster_centers_.shape)\n",
    "                # print(y_pred[0:9])\n",
    "\n",
    "                acc = np.round(Nmetrics.acc(y, y_pred), 5)\n",
    "                nmi = np.round(Nmetrics.nmi(y, y_pred), 5)\n",
    "                vmea = np.round(Nmetrics.vmeasure(y, y_pred), 5)\n",
    "                ari = np.round(Nmetrics.ari(y, y_pred), 5)\n",
    "                print('ACC=%.5f, NMI=%.5f, ARI=%.5f' % (acc, nmi, ari))\n",
    "                ACC.append(acc)\n",
    "                NMI.append(nmi)\n",
    "                ARI.append(ari)\n",
    "                the = np.sum(kl_loss) / len(x)\n",
    "                # print(kl_loss)\n",
    "                # print(Loss)\n",
    "                # print(np.sum(kl_loss), np.sum(Loss))\n",
    "\n",
    "                if y is not None:\n",
    "\n",
    "                    scale = len(y)\n",
    "                    for i in range(len(y)):\n",
    "                        predict = y_pred_sp[0][i]\n",
    "                        for view in range(len(x) - 1):\n",
    "                            if predict == y_pred_sp[view + 1][i]:\n",
    "                                continue\n",
    "                            else:\n",
    "                                scale -= 1\n",
    "                                break\n",
    "\n",
    "                    # alignment_before = alignment\n",
    "                    alignment = (scale / len(y))\n",
    "                    print('Aligned Ratio: %.2f%%. %d' % (alignment * 100, len(y)))\n",
    "                    Rate.append(alignment)\n",
    "                    tmpACC = []\n",
    "                    tmpNMI = []\n",
    "                    tmpARI = []\n",
    "                    for view in range(len(x)):\n",
    "                        acc = np.round(Nmetrics.acc(y, y_pred_sp[view]), 5)\n",
    "                        nmi = np.round(Nmetrics.nmi(y, y_pred_sp[view]), 5)\n",
    "                        vme = np.round(Nmetrics.vmeasure(y, y_pred_sp[view]), 5)\n",
    "                        ari = np.round(Nmetrics.ari(y, y_pred_sp[view]), 5)\n",
    "                        logdict = dict(iter=ite, nmi=nmi, vmea=vme, ari=ari, loss=avg_loss[view])\n",
    "                        logwriter.writerow(logdict)\n",
    "                        logfile.flush()\n",
    "                        print('V' + str(view + 1) + '-Iter %d: ACC=%.5f, NMI=%.5f, ARI=%.5f; Loss=%.5f' % (ite, acc, nmi, ari, avg_loss[view]))\n",
    "                        \n",
    "                        tmpACC.append(acc)\n",
    "                        tmpNMI.append(nmi)\n",
    "                        tmpARI.append(ari)\n",
    "                    vACC.append(tmpACC)\n",
    "                    vNMI.append(tmpNMI)\n",
    "                    vARI.append(tmpARI)\n",
    "                    ting = time() - t1\n",
    "\n",
    "                if alignment > arg.AR:\n",
    "                    alignment_large += 1\n",
    "                else:\n",
    "                    alignment_large = 0\n",
    "                # print(\"Over AR times:\" + str(alignment_large))\n",
    "                if alignment_large < arg.ARtime:\n",
    "                    Center_init = kmean.cluster_centers_    # k-means on global features\n",
    "                    new_P = self.new_P(z, Center_init)      # similarity measure\n",
    "                    print(\"Self-Supervised Multi-View Discriminative Feature Learning\")\n",
    "                    p = self.target_distribution(new_P)     # enhance discrimination\n",
    "                    center_init += 1\n",
    "                else:\n",
    "                    break\n",
    "                P = []\n",
    "                if arg.Coo == 1:\n",
    "                    print(\"Unified Target Distribution for Multiple KL Losses\")\n",
    "                    for view in range(len(x)):\n",
    "                        P.append(p)\n",
    "                else:\n",
    "                    print(\"self clustering\")\n",
    "                    for view in range(len(x)):\n",
    "                        P.append(self.target_distribution(Q_and_X[view * 2]))\n",
    "\n",
    "                # ge = np.random.randint(0, x[0].shape[0], 1, dtype=int)\n",
    "                # ge = int(ge)\n",
    "                # print('Number of sample:' + str(ge))\n",
    "                # for view in range(len(x)):\n",
    "                #     for i in Q_and_X[view * 2][ge]:\n",
    "                #         print(\"%.3f  \" % i, end=\"\")\n",
    "                #     print(\"\\n\")\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                for view in range(len(x)):\n",
    "                    Loss[view] = 0.\n",
    "                    kl_loss[view] = 0.\n",
    "\n",
    "            # train on batch\n",
    "            idx = index_array[index * batch_size: min((index + 1) * batch_size, x[0].shape[0])]\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for view in range(len(x)):\n",
    "                x_batch.append(x[view][idx])\n",
    "                y_batch.append(P[view][idx])\n",
    "                y_batch.append(x[view][idx])\n",
    "\n",
    "            tmp = self.train_on_batch(xin=x_batch, yout=y_batch)  # [sum, q, xn, q, x]\n",
    "            # print(tmp)\n",
    "            KLLoss = []\n",
    "            for view in range(len(x)):\n",
    "                Loss[view] += tmp[2 * view + 2]       # lr\n",
    "                kl_loss[view] += tmp[2 * view + 1]    # lc\n",
    "                # KLLoss += tmp[2 * view + 1]\n",
    "                KLLoss.append(tmp[2 * view + 1])\n",
    "            MVKLLoss.append(KLLoss)\n",
    "            index = index + 1 if (index + 1) * batch_size <= x[0].shape[0] else 0\n",
    "            # print(ite)\n",
    "            ite += 1\n",
    "            if ite >= int(maxiter):\n",
    "                ite = 0\n",
    "                # # Train from scratch\n",
    "                # print(\"Pretrain self.autoencoder\")\n",
    "                # optimizer = Adam(lr=0.001)\n",
    "                # self.pretrain(x, y, optimizer=optimizer, epochs=500, batch_size=batch_size,\n",
    "                #               save_dir=save_dir, verbose=1)\n",
    "                print(\"self.autoencoder.load_weights(args.pretrain_dir)\")\n",
    "                self.autoencoder.load_weights(arg.pretrain_dir)\n",
    "                features = self.encoder.predict(input_dic)\n",
    "                for view in range(len(x)):\n",
    "                    kmeans.fit_predict(features[view])\n",
    "                    self.model.get_layer(name='clustering' + str(view + 1)).set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/model_final.h5')\n",
    "        self.autoencoder.save_weights(save_dir + '/pre_model.h5')\n",
    "        # np.save(save_dir + '/AccNmiAriRate/ACC.npy', ACC)\n",
    "        # np.save(save_dir + '/AccNmiAriRate/NMI.npy', NMI)\n",
    "        # np.save(save_dir + '/AccNmiAriRate/ARI.npy', ARI)\n",
    "        # np.save(save_dir + '/AccNmiAriRate/vACC.npy', vACC)\n",
    "        # np.save(save_dir + '/AccNmiAriRate/vNMI.npy', vNMI)\n",
    "        # np.save(save_dir + '/AccNmiAriRate/vARI.npy', vARI)\n",
    "        # np.save(save_dir + '/AccNmiAriRate/Rate.npy', Rate)\n",
    "        # np.save(save_dir + '/AccNmiAriRate/MVKLLoss.npy', MVKLLoss)\n",
    "        print('Clustering time: %ds' % (time() - t1))\n",
    "        print('End clustering:', '-' * 60)\n",
    "\n",
    "        Q_and_X = self.model.predict(input_dic)\n",
    "        y_pred = []\n",
    "        for view in range(len(x)):\n",
    "            y_pred.append(Q_and_X[view * 2].argmax(1))\n",
    "\n",
    "        y_q = Q_and_X[(len(x) - 1) * 2]\n",
    "        for view in range(len(x) - 1):\n",
    "            y_q += Q_and_X[view * 2]\n",
    "        # y_q = y_q/len(x)\n",
    "        y_mean_pred = y_q.argmax(1)\n",
    "        return y_pred, y_mean_pred\n",
    "\n",
    "    def new_P(self, inputs, centers):\n",
    "        alpha = 1\n",
    "        q = 1.0 / (1.0 + (np.sum(np.square(np.expand_dims(inputs, axis=1) - centers), axis=2) / alpha))\n",
    "        q **= (alpha + 1.0) / 2.0\n",
    "        q = np.transpose(np.transpose(q) / np.sum(q, axis=1))\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557fef8",
   "metadata": {},
   "source": [
    "### Make Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a154218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_loss = []\n",
    "diversity_loss = []\n",
    "\n",
    "def _make_data_and_model(args):\n",
    "    # prepare dataset\n",
    "    x, y = load_data_conv(args.dataset)\n",
    "    view = len(x)\n",
    "    view_shapes = []\n",
    "         \n",
    "    Loss = []\n",
    "    Loss_weights = []\n",
    "    \n",
    "    for v in range(view):\n",
    "        view_shapes.append(x[v].shape[1:])\n",
    "        Loss_weights.append(args.lc) # -----Kld\n",
    "        Loss_weights.append(args.lm1) #-----Knn, Diversity, Elastic ------\n",
    "    \n",
    "    # prepare optimizer\n",
    "    optimizer = Adam(lr=args.lr)\n",
    "    # prepare the model\n",
    "    n_clusters = len(np.unique(y))\n",
    "    # n_clusters = 40   # over clustering\n",
    "    print(\"n_clusters:\" + str(n_clusters))\n",
    "    # lc = 0.1\n",
    "    \n",
    "    print('K of Knn: ', k)\n",
    "    print('Loss Weight: ', args.lm1)\n",
    "\n",
    "    model = MvDEC(filters=[32, 64, 128, 10], n_clusters=n_clusters, view_shape=view_shapes)\n",
    "\n",
    "    # get feature space from autoencoders\n",
    "    \n",
    "    encoder = model.encoders\n",
    "    feature_space_matrix = []\n",
    "    \n",
    "    lam_Knn = 0.01\n",
    "    lam_Div = 0.0001\n",
    "    \n",
    "    print('Lambda Knn: ', lam_Knn, ',  Lambda Div: ', lam_Div)\n",
    "\n",
    "    # create feature matrix of encoder for each view\n",
    "\n",
    "    if data == 'MNIST_USPS' or data == 'BDGP':  #2 views\n",
    "        feature_space_matrix.append(encoder[0].predict(x[0]))\n",
    "        feature_space_matrix.append(encoder[1].predict(x[1]))\n",
    "    \n",
    "    elif data == 'Fashion_MV':  #3 views\n",
    "        feature_space_matrix.append(encoder[0].predict(x[0]))\n",
    "        feature_space_matrix.append(encoder[1].predict(x[1]))\n",
    "        feature_space_matrix.append(encoder[2].predict(x[2]))\n",
    "        \n",
    "    elif data == 'Caltech101_20':  #6 views\n",
    "        feature_space_matrix.append(encoder[0].predict(x[0]))\n",
    "        feature_space_matrix.append(encoder[1].predict(x[1]))\n",
    "        feature_space_matrix.append(encoder[2].predict(x[2]))\n",
    "        feature_space_matrix.append(encoder[3].predict(x[3]))\n",
    "        feature_space_matrix.append(encoder[4].predict(x[4]))\n",
    "        feature_space_matrix.append(encoder[5].predict(x[5]))\n",
    "         \n",
    "    print(\"###############################\")    \n",
    "    print('encoder[0]:', feature_space_matrix[0].shape)\n",
    "    print('encoder[1]:', feature_space_matrix[1].shape)\n",
    "    print(\"###############################\")\n",
    "    \n",
    "    # trace of dot multiplication between laplacian_matrix and feature_space_matrix\n",
    "\n",
    "    if data == 'BDGP':   #2 views\n",
    "        l_f_1 = np.dot(laplacian_matrix1,feature_space_matrix[0])\n",
    "        l_f_2 = np.dot(laplacian_matrix2,feature_space_matrix[1])\n",
    "\n",
    "        graph_KNN_loss1 = l_f_1.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss1)\n",
    "\n",
    "        graph_KNN_loss2 = l_f_2.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss2)\n",
    "    \n",
    "    elif data == 'MNIST_USPS':  #2 views\n",
    "        #reshape\n",
    "        feature_space_matrix[0] = feature_space_matrix[0].reshape((feature_space_matrix[0].shape[0], -1))\n",
    "        feature_space_matrix[1] = feature_space_matrix[1].reshape((feature_space_matrix[1].shape[0], -1))\n",
    "        \n",
    "        l_f_1 = np.dot(laplacian_matrix1,feature_space_matrix[0])\n",
    "        l_f_2 = np.dot(laplacian_matrix2,feature_space_matrix[1])\n",
    "\n",
    "        graph_KNN_loss1 = l_f_1.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss1)\n",
    "\n",
    "        graph_KNN_loss2 = l_f_2.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss2)\n",
    "    \n",
    "    elif data == 'Fashion_MV':  #3 views\n",
    "        #reshape\n",
    "        feature_space_matrix[0] = feature_space_matrix[0].reshape((feature_space_matrix[0].shape[0], -1))\n",
    "        feature_space_matrix[1] = feature_space_matrix[1].reshape((feature_space_matrix[1].shape[0], -1))\n",
    "        feature_space_matrix[2] = feature_space_matrix[2].reshape((feature_space_matrix[2].shape[0], -1))\n",
    "        \n",
    "        l_f_1 = np.dot(laplacian_matrix1,feature_space_matrix[0])\n",
    "        l_f_2 = np.dot(laplacian_matrix2,feature_space_matrix[1])\n",
    "        l_f_3 = np.dot(laplacian_matrix3,feature_space_matrix[2])\n",
    "\n",
    "        graph_KNN_loss1 = l_f_1.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss1)\n",
    "\n",
    "        graph_KNN_loss2 = l_f_2.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss2)\n",
    "        \n",
    "        graph_KNN_loss3 = l_f_3.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss3)\n",
    "        \n",
    "    elif data == 'Caltech101_20':  #6 views\n",
    "        l_f_1 = np.dot(laplacian_matrix1,feature_space_matrix[0])\n",
    "        l_f_2 = np.dot(laplacian_matrix2,feature_space_matrix[1])\n",
    "        l_f_3 = np.dot(laplacian_matrix3,feature_space_matrix[2])\n",
    "        l_f_4 = np.dot(laplacian_matrix4,feature_space_matrix[3])\n",
    "        l_f_5 = np.dot(laplacian_matrix5,feature_space_matrix[4])\n",
    "        l_f_6 = np.dot(laplacian_matrix6,feature_space_matrix[5])\n",
    "\n",
    "        graph_KNN_loss1 = l_f_1.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss1)\n",
    "\n",
    "        graph_KNN_loss2 = l_f_2.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss2)\n",
    "        \n",
    "        graph_KNN_loss3 = l_f_3.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss3)\n",
    "                \n",
    "        graph_KNN_loss4 = l_f_4.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss4)\n",
    "\n",
    "        graph_KNN_loss5 = l_f_5.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss5)\n",
    "        \n",
    "        graph_KNN_loss6 = l_f_6.trace()\n",
    "        KNN_loss.append(lam_Knn * graph_KNN_loss6)\n",
    "    \n",
    "#     print(\"--------- KNN_loss ---------------\")\n",
    "    \n",
    "#     for i in range(len(KNN_loss)):\n",
    "#         print(KNN_loss[i])\n",
    "    \n",
    "#     print(\"----------------------------------\")\n",
    "    \n",
    "    \n",
    "# Loss Diversity\n",
    "        \n",
    "    for i in range(len(feature_space_matrix)):\n",
    "        \n",
    "        if data == 'MNIST_USPS':\n",
    "            sum_v = np.zeros((5000, 5000))\n",
    "        \n",
    "        elif data == 'Fashion_MV':\n",
    "            sum_v = np.zeros((10000, 10000))\n",
    "        \n",
    "        elif data == 'BDGP':\n",
    "            sum_v = np.zeros((2500, 2500))\n",
    "            \n",
    "        elif data == 'Caltech101_20':\n",
    "            sum_v = np.zeros((2386, 2386))\n",
    "            \n",
    "        for j in range(len(feature_space_matrix)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            else:\n",
    "                arr = np.dot(feature_space_matrix[i], tf.transpose(feature_space_matrix[j]))\n",
    "                sum_v = sum_v + arr\n",
    "\n",
    "        sum_trace = sum_v.trace()\n",
    "        diversity_loss.append(lam_Div * sum_trace)\n",
    "            \n",
    "#     print(\"............... loss_diversity ......................\")\n",
    "    \n",
    "#     for i in range(len(diversity_loss)):\n",
    "#         print(diversity_loss[i])\n",
    "    \n",
    "#     print(\"......................................................\")\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=total_loss, loss_weights=Loss_weights)\n",
    "    \n",
    "    return x, y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba71693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # get data and mode\n",
    "    x, y, model = _make_data_and_model(args)\n",
    "\n",
    "    model.model.summary()\n",
    "    \n",
    "    # pretraining\n",
    "    t0 = time()\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    if args.pretrain_dir is not None and os.path.exists(args.pretrain_dir):  # load pretrained weights\n",
    "        model.autoencoder.load_weights(args.pretrain_dir)\n",
    "        # model.load_weights(args.pretrain_dir)\n",
    "    else:  # train\n",
    "        optimizer = Adam(lr=args.lr)\n",
    "        model.pretrain(x, y, optimizer=optimizer, epochs=args.pretrain_epochs,\n",
    "                            batch_size=args.batch_size, save_dir=args.save_dir, verbose=args.verbose)\n",
    "        args.pretrain_dir = args.save_dir + '/ae_weights.h5'\n",
    "    t1 = time()\n",
    "    print(\"Time for pretraining: %ds\" % (t1 - t0))\n",
    "\n",
    "    # clustering\n",
    "    # DEMVC, IDEC, DEC\n",
    "    # y_pred, y_mean_pred = model.fit(arg=args, x=x, y=y, maxiter=args.maxiter,\n",
    "    #                                            batch_size=args.batch_size, UpdateCoo=args.UpdateCoo,\n",
    "    #                                            save_dir=args.save_dir)\n",
    "    # SDMVC\n",
    "    y_pred, y_mean_pred = model.new_fit(arg=args, x=x, y=y, maxiter=args.maxiter,\n",
    "                                    batch_size=args.batch_size, UpdateCoo=args.UpdateCoo,\n",
    "                                    save_dir=args.save_dir)\n",
    "    if y is not None:\n",
    "        for view in range(len(x)):\n",
    "            print('Final: acc=%.4f, nmi=%.4f, ari=%.4f' %\n",
    "                    (Nmetrics.acc(y, y_pred[view]), Nmetrics.nmi(y, y_pred[view]), Nmetrics.ari(y, y_pred[view])))\n",
    "        print('Final: acc=%.4f, nmi=%.4f, ari=%.4f' %\n",
    "                  (Nmetrics.acc(y, y_mean_pred), Nmetrics.nmi(y, y_mean_pred), Nmetrics.ari(y, y_mean_pred)))\n",
    "\n",
    "    t2 = time()\n",
    "    print(\"Time for pretaining, clustering and total: (%ds, %ds, %ds)\" % (t1 - t0, t2 - t1, t2 - t0))\n",
    "    print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "319ec3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args):\n",
    "    assert args.weights is not None\n",
    "\n",
    "    x, y, model = _make_data_and_model(args)\n",
    "    model.model.summary()\n",
    "    print('Begin testing:', '-' * 60)\n",
    "    model.load_weights(args.weights)\n",
    "    y_pred, y_mean_pred = model.predict_label(x=x)\n",
    "    if y is not None:\n",
    "        for view in range(len(x)):\n",
    "            print('Final: acc=%.4f, nmi=%.4f, ari=%.4f' %\n",
    "                    (Nmetrics.acc(y, y_pred[view]), Nmetrics.nmi(y, y_pred[view]), Nmetrics.ari(y, y_pred[view])))\n",
    "        print('Final: acc=%.4f, nmi=%.4f, ari=%.4f' %\n",
    "                  (Nmetrics.acc(y, y_mean_pred), Nmetrics.nmi(y, y_mean_pred), Nmetrics.ari(y, y_mean_pred)))\n",
    "    \n",
    "    print('End testing:', '-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5192b1",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e953d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load: Caltech101_20\n",
      "(2386, 48)\n",
      "(2386, 40)\n",
      "(2386, 254)\n",
      "(2386, 1984)\n",
      "(2386, 512)\n",
      "(2386, 928)\n",
      "(2386,)\n"
     ]
    }
   ],
   "source": [
    "# data = 'BDGP'\n",
    "data = 'MNIST_USPS'\n",
    "# data = 'Fashion_MV'\n",
    "# data = 'Caltech101_20'\n",
    "\n",
    "x, y = load_data_conv(data)\n",
    "k = 3\n",
    "#reshape for mnist and fashion dataset\n",
    "if data == 'MNIST_USPS':\n",
    "    x[0] = x[0].reshape((x[0].shape[0], -1))\n",
    "    x[1] = x[1].reshape((x[1].shape[0], -1))\n",
    "    knn_m1 = kneighbors_graph(x[0], k, metric='cosine', mode='distance', include_self=True)\n",
    "    knn_m2 = kneighbors_graph(x[1], k, metric='cosine', mode='distance', include_self=True)\n",
    "\n",
    "elif data == 'Fashion_MV':\n",
    "    x[0] = x[0].reshape((x[0].shape[0], -1))\n",
    "    x[1] = x[1].reshape((x[1].shape[0], -1))\n",
    "    x[2] = x[2].reshape((x[2].shape[0], -1))\n",
    "    knn_m1 = kneighbors_graph(x[0], k, metric='cosine', mode='distance', include_self=True)\n",
    "    knn_m2 = kneighbors_graph(x[1], k, metric='cosine', mode='distance', include_self=True)\n",
    "    knn_m3 = kneighbors_graph(x[2], k, metric='cosine', mode='distance', include_self=True)\n",
    "\n",
    "elif data == 'BDGP':\n",
    "    knn_m1 = kneighbors_graph(x[0], k, metric='cosine', mode='distance', include_self=True)\n",
    "    knn_m2 = kneighbors_graph(x[1], k, metric='cosine', mode='distance', include_self=True)\n",
    "\n",
    "elif data == 'Caltech101_20':\n",
    "    knn_m1 = kneighbors_graph(x[0], k, metric='cosine', mode='distance', include_self=True)\n",
    "    knn_m2 = kneighbors_graph(x[1], k, metric='cosine', mode='distance', include_self=True)\n",
    "    knn_m3 = kneighbors_graph(x[2], k, metric='cosine', mode='distance', include_self=True)\n",
    "    knn_m4 = kneighbors_graph(x[3], k, metric='cosine', mode='distance', include_self=True)\n",
    "    knn_m5 = kneighbors_graph(x[4], k, metric='cosine', mode='distance', include_self=True)\n",
    "    knn_m6 = kneighbors_graph(x[5], k, metric='cosine', mode='distance', include_self=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880aaaf",
   "metadata": {},
   "source": [
    "## Degree Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daaec74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data == 'MNIST_USPS' or data == 'BDGP':  #2 views\n",
    "    degree1 = np.diag(np.sum(knn_m1.toarray(), axis=1))\n",
    "    degree2 = np.diag(np.sum(knn_m2.toarray(), axis=1))\n",
    "\n",
    "elif data == 'Fashion_MV':  #3 views\n",
    "    degree1 = np.diag(np.sum(knn_m1.toarray(), axis=1))\n",
    "    degree2 = np.diag(np.sum(knn_m2.toarray(), axis=1))\n",
    "    degree3 = np.diag(np.sum(knn_m3.toarray(), axis=1))\n",
    "    \n",
    "elif data == 'Caltech101_20':  #6 views\n",
    "    degree1 = np.diag(np.sum(knn_m1.toarray(), axis=1))\n",
    "    degree2 = np.diag(np.sum(knn_m2.toarray(), axis=1))\n",
    "    degree3 = np.diag(np.sum(knn_m3.toarray(), axis=1))\n",
    "    degree4 = np.diag(np.sum(knn_m4.toarray(), axis=1))\n",
    "    degree5 = np.diag(np.sum(knn_m5.toarray(), axis=1))\n",
    "    degree6 = np.diag(np.sum(knn_m6.toarray(), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4dd357",
   "metadata": {},
   "source": [
    "## Laplacian Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b436d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data == 'MNIST_USPS' or data == 'BDGP':\n",
    "    laplacian_matrix1 = degree1 - knn_m1\n",
    "    laplacian_matrix2 = degree2 - knn_m2\n",
    "    \n",
    "elif data == 'Fashion_MV':\n",
    "    laplacian_matrix1 = degree1 - knn_m1\n",
    "    laplacian_matrix2 = degree2 - knn_m2\n",
    "    laplacian_matrix3 = degree3 - knn_m3\n",
    "    \n",
    "elif data == 'Caltech101_20':\n",
    "    laplacian_matrix1 = degree1 - knn_m1\n",
    "    laplacian_matrix2 = degree2 - knn_m2\n",
    "    laplacian_matrix3 = degree3 - knn_m3    \n",
    "    laplacian_matrix4 = degree4 - knn_m4\n",
    "    laplacian_matrix5 = degree5 - knn_m5\n",
    "    laplacian_matrix6 = degree6 - knn_m6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b196d",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67430791",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d6607ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "\n",
    "    # Calculation of Mean Squared Error (MSE)\n",
    "    ml = tf.keras.losses.MeanSquaredError()\n",
    "    M_Loss = ml(y_true, y_pred)\n",
    "    \n",
    "    return M_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb6cfa6",
   "metadata": {},
   "source": [
    "### Kullback-Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dcc3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kld_loss(y_true, y_pred):\n",
    "\n",
    "    # Computes Kullback-Leibler divergence loss (Kld)    \n",
    "    kl = tf.keras.losses.KLDivergence()\n",
    "    K_Loss = kl(y_true, y_pred)\n",
    "\n",
    "    return K_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ed5dd3",
   "metadata": {},
   "source": [
    "### Elastic loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1838f830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta:  100\n"
     ]
    }
   ],
   "source": [
    "delta = 100\n",
    "print(\"Delta: \", delta)\n",
    "\n",
    "def elastic_loss(y_true, y_pred):\n",
    "\n",
    "    # Computes Elastic loss (El)\n",
    "    \n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    a = tf.square(tf.subtract(y_true, y_pred))\n",
    "    b = (delta*a) / (delta+a)\n",
    "    c = a / (a+delta)\n",
    "    \n",
    "    d = b + c\n",
    "    el_Loss = tf.reduce_sum(d)\n",
    "\n",
    "    return el_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a28b014",
   "metadata": {},
   "source": [
    "### Total Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c30cda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(y_true, y_pred):\n",
    "        \n",
    "    totalloss = []\n",
    "    \n",
    "    for v in range(len(x)):\n",
    "        totalloss.append(kld_loss(y_true, y_pred))\n",
    "#         totalloss.append(mse_loss(y_true, y_pred))\n",
    "        totalloss.append(elastic_loss(y_true, y_pred) + KNN_loss[v].item() + diversity_loss[v].item())\n",
    "#         totalloss.append(KNN_loss[v].item())\n",
    "#         totalloss.append(diversity_loss[v].item())\n",
    "\n",
    "    return totalloss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bcc0ec",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fef13918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv=['']\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "623ebfda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++  Parameters  ++++++++++++++++++++++++++++++\n",
      "Namespace(dataset='Caltech101_20', save_dir='./results/Caltech101_20', pretrain_dir=None, pretrain_epochs=1, verbose=1, testing=False, weights=None, lr=0.001, batch_size=256, maxiter=30000, UpdateCoo=1000, view_first=1, Coo=1, K12q=0, Idec=1.0, lc=0.1, AR=0.9, ARtime=1, lm1=0.1)\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "load: Caltech101_20\n",
      "(2386, 48)\n",
      "(2386, 40)\n",
      "(2386, 254)\n",
      "(2386, 1984)\n",
      "(2386, 512)\n",
      "(2386, 928)\n",
      "(2386,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 11:13:20.501388: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters:20\n",
      "K of Knn:  3\n",
      "Loss Weight:  0.1\n",
      "Lambda Knn:  0.01 ,  Lambda Div:  0.0001\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "###############################\n",
      "encoder[0]: (2386, 100)\n",
      "encoder[1]: (2386, 100)\n",
      "###############################\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input1 (InputLayer)         [(None, 48)]                 0         []                            \n",
      "                                                                                                  \n",
      " input2 (InputLayer)         [(None, 40)]                 0         []                            \n",
      "                                                                                                  \n",
      " input3 (InputLayer)         [(None, 254)]                0         []                            \n",
      "                                                                                                  \n",
      " input4 (InputLayer)         [(None, 1984)]               0         []                            \n",
      "                                                                                                  \n",
      " input5 (InputLayer)         [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " input6 (InputLayer)         [(None, 928)]                0         []                            \n",
      "                                                                                                  \n",
      " v1_encoder_0 (Dense)        (None, 500)                  24500     ['input1[0][0]']              \n",
      "                                                                                                  \n",
      " v2_encoder_0 (Dense)        (None, 500)                  20500     ['input2[0][0]']              \n",
      "                                                                                                  \n",
      " v3_encoder_0 (Dense)        (None, 500)                  127500    ['input3[0][0]']              \n",
      "                                                                                                  \n",
      " v4_encoder_0 (Dense)        (None, 500)                  992500    ['input4[0][0]']              \n",
      "                                                                                                  \n",
      " v5_encoder_0 (Dense)        (None, 500)                  256500    ['input5[0][0]']              \n",
      "                                                                                                  \n",
      " v6_encoder_0 (Dense)        (None, 500)                  464500    ['input6[0][0]']              \n",
      "                                                                                                  \n",
      " v1_encoder_1 (Dense)        (None, 500)                  250500    ['v1_encoder_0[0][0]']        \n",
      "                                                                                                  \n",
      " v2_encoder_1 (Dense)        (None, 500)                  250500    ['v2_encoder_0[0][0]']        \n",
      "                                                                                                  \n",
      " v3_encoder_1 (Dense)        (None, 500)                  250500    ['v3_encoder_0[0][0]']        \n",
      "                                                                                                  \n",
      " v4_encoder_1 (Dense)        (None, 500)                  250500    ['v4_encoder_0[0][0]']        \n",
      "                                                                                                  \n",
      " v5_encoder_1 (Dense)        (None, 500)                  250500    ['v5_encoder_0[0][0]']        \n",
      "                                                                                                  \n",
      " v6_encoder_1 (Dense)        (None, 500)                  250500    ['v6_encoder_0[0][0]']        \n",
      "                                                                                                  \n",
      " v1_encoder_2 (Dense)        (None, 2000)                 1002000   ['v1_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " v2_encoder_2 (Dense)        (None, 2000)                 1002000   ['v2_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " v3_encoder_2 (Dense)        (None, 2000)                 1002000   ['v3_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " v4_encoder_2 (Dense)        (None, 2000)                 1002000   ['v4_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " v5_encoder_2 (Dense)        (None, 2000)                 1002000   ['v5_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " v6_encoder_2 (Dense)        (None, 2000)                 1002000   ['v6_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " embedding1 (Dense)          (None, 100)                  200100    ['v1_encoder_2[0][0]']        \n",
      "                                                                                                  \n",
      " embedding2 (Dense)          (None, 100)                  200100    ['v2_encoder_2[0][0]']        \n",
      "                                                                                                  \n",
      " embedding3 (Dense)          (None, 100)                  200100    ['v3_encoder_2[0][0]']        \n",
      "                                                                                                  \n",
      " embedding4 (Dense)          (None, 100)                  200100    ['v4_encoder_2[0][0]']        \n",
      "                                                                                                  \n",
      " embedding5 (Dense)          (None, 100)                  200100    ['v5_encoder_2[0][0]']        \n",
      "                                                                                                  \n",
      " embedding6 (Dense)          (None, 100)                  200100    ['v6_encoder_2[0][0]']        \n",
      "                                                                                                  \n",
      " v1_decoder_3 (Dense)        (None, 2000)                 202000    ['embedding1[0][0]']          \n",
      "                                                                                                  \n",
      " v2_decoder_3 (Dense)        (None, 2000)                 202000    ['embedding2[0][0]']          \n",
      "                                                                                                  \n",
      " v3_decoder_3 (Dense)        (None, 2000)                 202000    ['embedding3[0][0]']          \n",
      "                                                                                                  \n",
      " v4_decoder_3 (Dense)        (None, 2000)                 202000    ['embedding4[0][0]']          \n",
      "                                                                                                  \n",
      " v5_decoder_3 (Dense)        (None, 2000)                 202000    ['embedding5[0][0]']          \n",
      "                                                                                                  \n",
      " v6_decoder_3 (Dense)        (None, 2000)                 202000    ['embedding6[0][0]']          \n",
      "                                                                                                  \n",
      " v1_decoder_2 (Dense)        (None, 500)                  1000500   ['v1_decoder_3[0][0]']        \n",
      "                                                                                                  \n",
      " v2_decoder_2 (Dense)        (None, 500)                  1000500   ['v2_decoder_3[0][0]']        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " v3_decoder_2 (Dense)        (None, 500)                  1000500   ['v3_decoder_3[0][0]']        \n",
      "                                                                                                  \n",
      " v4_decoder_2 (Dense)        (None, 500)                  1000500   ['v4_decoder_3[0][0]']        \n",
      "                                                                                                  \n",
      " v5_decoder_2 (Dense)        (None, 500)                  1000500   ['v5_decoder_3[0][0]']        \n",
      "                                                                                                  \n",
      " v6_decoder_2 (Dense)        (None, 500)                  1000500   ['v6_decoder_3[0][0]']        \n",
      "                                                                                                  \n",
      " v1_decoder_1 (Dense)        (None, 500)                  250500    ['v1_decoder_2[0][0]']        \n",
      "                                                                                                  \n",
      " v2_decoder_1 (Dense)        (None, 500)                  250500    ['v2_decoder_2[0][0]']        \n",
      "                                                                                                  \n",
      " v3_decoder_1 (Dense)        (None, 500)                  250500    ['v3_decoder_2[0][0]']        \n",
      "                                                                                                  \n",
      " v4_decoder_1 (Dense)        (None, 500)                  250500    ['v4_decoder_2[0][0]']        \n",
      "                                                                                                  \n",
      " v5_decoder_1 (Dense)        (None, 500)                  250500    ['v5_decoder_2[0][0]']        \n",
      "                                                                                                  \n",
      " v6_decoder_1 (Dense)        (None, 500)                  250500    ['v6_decoder_2[0][0]']        \n",
      "                                                                                                  \n",
      " clustering1 (ClusteringLay  (None, 20)                   2000      ['embedding1[0][0]']          \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " v1_decoder_0 (Dense)        (None, 48)                   24048     ['v1_decoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " clustering2 (ClusteringLay  (None, 20)                   2000      ['embedding2[0][0]']          \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " v2_decoder_0 (Dense)        (None, 40)                   20040     ['v2_decoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " clustering3 (ClusteringLay  (None, 20)                   2000      ['embedding3[0][0]']          \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " v3_decoder_0 (Dense)        (None, 254)                  127254    ['v3_decoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " clustering4 (ClusteringLay  (None, 20)                   2000      ['embedding4[0][0]']          \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " v4_decoder_0 (Dense)        (None, 1984)                 993984    ['v4_decoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " clustering5 (ClusteringLay  (None, 20)                   2000      ['embedding5[0][0]']          \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " v5_decoder_0 (Dense)        (None, 512)                  256512    ['v5_decoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " clustering6 (ClusteringLay  (None, 20)                   2000      ['embedding6[0][0]']          \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " v6_decoder_0 (Dense)        (None, 928)                  464928    ['v6_decoder_1[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21218366 (80.94 MB)\n",
      "Trainable params: 21218366 (80.94 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin pretraining:  ------------------------------------------------------------\n",
      "75/75 [==============================] - 0s 2ms/steposs: 0.7339 - v1_decoder_0_loss: 0.0359 - v2_decoder_0_loss: 0.2002 - v3_decoder_0_loss: 0.0665 - v4_decoder_0_loss: 0.2413 - v5_decoder_0_loss: 0.0834 - v6_decoder_0_loss: \n",
      "\n",
      "        |==>  acc: 0.1324,  nmi: 0.1030  <==|\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "\n",
      "        |==>  acc: 0.2234,  nmi: 0.2875  <==|\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "\n",
      "        |==>  acc: 0.1764,  nmi: 0.1940  <==|\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "\n",
      "        |==>  acc: 0.2163,  nmi: 0.3300  <==|\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "\n",
      "        |==>  acc: 0.1697,  nmi: 0.1781  <==|\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "\n",
      "        |==>  acc: 0.1945,  nmi: 0.2077  <==|\n",
      "10/10 [==============================] - 12s 787ms/step - loss: 0.7339 - v1_decoder_0_loss: 0.0359 - v2_decoder_0_loss: 0.2002 - v3_decoder_0_loss: 0.0665 - v4_decoder_0_loss: 0.2413 - v5_decoder_0_loss: 0.0834 - v6_decoder_0_loss: 0.1066\n",
      "Pretraining time:  11.764742374420166\n",
      "Pretrained weights are saved to ./results/Caltech101_20/ae_weights.h5\n",
      "End pretraining:  ------------------------------------------------------------\n",
      "Time for pretraining: 12s\n",
      "Begin clustering: ------------------------------------------------------------\n",
      "Update Coo: 1000\n",
      "Save interval 30000\n",
      "Initializing cluster centers with k-means.\n",
      "75/75 [==============================] - 1s 7ms/step\n",
      "Start-1: acc=0.13202, nmi=0.10426, v-measure=0.10426, ari=0.03615\n",
      "Start-2: acc=0.22339, nmi=0.28164, v-measure=0.28164, ari=0.17105\n",
      "Start-3: acc=0.18064, nmi=0.19391, v-measure=0.19391, ari=0.07075\n",
      "Start-4: acc=0.22716, nmi=0.33914, v-measure=0.33914, ari=0.15796\n",
      "Start-5: acc=0.16094, nmi=0.15181, v-measure=0.15181, ari=0.04219\n",
      "Start-6: acc=0.19489, nmi=0.20872, v-measure=0.20872, ari=0.06039\n",
      "\n",
      "\n",
      "75/75 [==============================] - 1s 11ms/step\n",
      "75/75 [==============================] - 1s 7ms/step\n",
      "ACC=0.30469, NMI=0.37186, ARI=0.21197\n",
      "Aligned Ratio: 0.00%. 2386\n",
      "V1-Iter 0: ACC=0.13202, NMI=0.10426, ARI=0.03615; Loss=0.00000\n",
      "V2-Iter 0: ACC=0.22339, NMI=0.28164, ARI=0.17105; Loss=0.00000\n",
      "V3-Iter 0: ACC=0.18064, NMI=0.19391, ARI=0.07075; Loss=0.00000\n",
      "V4-Iter 0: ACC=0.22716, NMI=0.33914, ARI=0.15796; Loss=0.00000\n",
      "V5-Iter 0: ACC=0.16094, NMI=0.15181, ARI=0.04219; Loss=0.00000\n",
      "V6-Iter 0: ACC=0.19489, NMI=0.20872, ARI=0.06039; Loss=0.00000\n",
      "Self-Supervised Multi-View Discriminative Feature Learning\n",
      "Unified Target Distribution for Multiple KL Losses\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m     test(args)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime for pretraining: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (t1 \u001b[38;5;241m-\u001b[39m t0))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# clustering\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# DEMVC, IDEC, DEC\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# y_pred, y_mean_pred = model.fit(arg=args, x=x, y=y, maxiter=args.maxiter,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#                                            batch_size=args.batch_size, UpdateCoo=args.UpdateCoo,\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#                                            save_dir=args.save_dir)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# SDMVC\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m y_pred, y_mean_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUpdateCoo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUpdateCoo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m view \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x)):\n",
      "Cell \u001b[0;32mIn[10], line 598\u001b[0m, in \u001b[0;36mMvDEC.new_fit\u001b[0;34m(self, arg, x, y, maxiter, batch_size, tol, UpdateCoo, save_dir)\u001b[0m\n\u001b[1;32m    596\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m view \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x)):\n\u001b[0;32m--> 598\u001b[0m     \u001b[43mx_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mview\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m     y_batch\u001b[38;5;241m.\u001b[39mappend(P[view][idx])\n\u001b[1;32m    600\u001b[0m     y_batch\u001b[38;5;241m.\u001b[39mappend(x[view][idx])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # -------------------------------------------------------\n",
    "    # Dataset settings\n",
    "    # 'MNIST_USPS'     # 2 views, 10 clusters,  5000 examples\n",
    "    # 'Fashion_MV'     # 3 views, 10 clusters, 10000 examples\n",
    "    # 'BDGP'           # 2 views,  5 clusters,  2500 examples\n",
    "    # 'Caltech101_20'  # 6 views, 20 clusters,  2386 examples\n",
    "    # -------------------------------------------------------\n",
    "    data = 'MNIST_USPS'\n",
    "#     TEST = True        # Test the clustering performance of the trained models\n",
    "    TEST = False\n",
    "    Train_ae = True  # The stability of AEs pre-training and K-means might be the bottleneck for AE/K-means based MVC\n",
    "#     Train_ae = False   # The reported results are the average values after pre-training\n",
    "\n",
    "    AR = 0.90          # Aligned Ratio, e.g., 90%\n",
    "    Coo = 1            # Unified P\n",
    "    View = 1           # View_first SetC for DEMVC\n",
    "    # K123q = View     # DEMVC\n",
    "    K123q = 0          # SDMVC\n",
    "    if Coo == 0:\n",
    "        K123q = 0      # K-means 1k1 , 2k2, 3k3, 0: k-means, >view number: no settings centers\n",
    "\n",
    "    epochs = 500       # 500 epochs for pre-training AEs\n",
    "    Update_Coo = 1000  # Iterations to update self-supervised objective\n",
    "    Maxiter = 30000    # Max iterations for DEC, IDEC or DEMVC, not for SDMVC\n",
    "    Batch = 256        # Batch size\n",
    "    lc = 0.1           # Clustering loss = 0.1 --------- Kullback-Leibler\n",
    "    Idec = 1.0         # Dec 0.0 , Idec 1.0 --------  Reconstruction loss 1.0 -------- MSE or ELASTIC\n",
    "    lrate = 0.001      # Learning rate = 0.001 ---- keras defult\n",
    "\n",
    "    lm_loss = 0.1      # weight loss = 0.1\n",
    "    \n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='main')\n",
    "\n",
    "    parser.add_argument('--dataset', default=data,\n",
    "                        help=\"Dataset name to train on\")\n",
    "    PATH = './results/'\n",
    "    path = PATH + data\n",
    "    if Train_ae:\n",
    "        load = None\n",
    "    else:\n",
    "        load = path + '/ae_weights.h5'\n",
    "    if TEST:\n",
    "        load_test = path + '/model_final.h5'\n",
    "    else:\n",
    "        load_test = None\n",
    "                        \n",
    "    parser.add_argument('-d', '--save-dir', default=path,\n",
    "                        help=\"Dir to save the results\")\n",
    "    # Parameters for pretraining\n",
    "    parser.add_argument('--pretrain_dir', default=load, type=str,\n",
    "                        help=\"Pretrained weights of the autoencoder\")\n",
    "    parser.add_argument('--pretrain-epochs', default=epochs, type=int,   # 500\n",
    "                        help=\"Number of epochs for pretraining\")\n",
    "    parser.add_argument('-v', '--verbose', default=1, type=int,\n",
    "                        help=\"Verbose for pretraining\")\n",
    "    # Parameters for clustering\n",
    "    parser.add_argument('--testing', default=TEST, type=bool,\n",
    "                        help=\"Testing the clustering performance with provided weights\")\n",
    "    parser.add_argument('--weights', default=load_test, type=str,\n",
    "                        help=\"Model weights, used for testing\")\n",
    "    # pretrain_optimizer = 'adam'   # adam, sgd\n",
    "    # parser.add_argument('--optimizer', default=pretrain_optimizer, type=str,\n",
    "    #                     help=\"Optimizer for clustering phase\")\n",
    "    parser.add_argument('--lr', default=lrate, type=float,\n",
    "                        help=\"learning rate during clustering\")\n",
    "    parser.add_argument('--batch-size', default=Batch, type=int,   # 256\n",
    "                        help=\"Batch size\")\n",
    "    parser.add_argument('--maxiter', default=Maxiter, type=int,    # 2e4\n",
    "                        help=\"Maximum number of iterations\")\n",
    "    parser.add_argument('-uc', '--UpdateCoo', default=Update_Coo, type=int,   # 200 \n",
    "                        help=\"Number of iterations to update the target distribution\")\n",
    "    parser.add_argument('--view_first', default=View, type=int,\n",
    "                        help=\"view-first\")\n",
    "    parser.add_argument('--Coo', default=Coo, type=int,\n",
    "                        help=\"Coo?\")\n",
    "    parser.add_argument('--K12q', default=K123q, type=int,\n",
    "                        help=\"Kmeans\")\n",
    "    parser.add_argument('--Idec', default=Idec, type=float,\n",
    "                        help=\"dec?\")\n",
    "    parser.add_argument('--lc', default=lc, type=float,\n",
    "                        help=\"Idec?\")\n",
    "    parser.add_argument('--AR', default=AR, type=float,\n",
    "                        help=\"aligned rate?\")\n",
    "    parser.add_argument('--ARtime', default=1, type=float,\n",
    "                        help=\"over aligned rate times?\")\n",
    "    \n",
    "    parser.add_argument('--lm1', default=lm_loss, type=float,\n",
    "                        help=\"loss\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    print('+' * 30, ' Parameters ', '+' * 30)\n",
    "    print(args)\n",
    "    print('+' * 75)\n",
    "    \n",
    "    # testing\n",
    "    if args.testing:\n",
    "        test(args)\n",
    "    else:\n",
    "        train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38d94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment1",
   "language": "python",
   "name": "environment1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
